{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Recommendations based on ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems rely on different types of input. Most convenient is the high quality explicit feedback, which includes explicit input by users regarding their interest in products. For example, Netflix collects star ratings for movies and TiVo users indicate their preferences for TV shows by hitting thumbs-up/down buttons. However, explicit feedback is not always available. Thus, recommenders can infer user preferences from the more abundant implicit feedback, which indirectly reflect opinion through observing user behavior. Types of implicit feedback include purchase history, browsing history, search patterns, or even mouse movements.\n",
    "\n",
    "The vast majority of the literature in the field is focused on processing explicit feedback; probably thanks to the con venience of using this kind of pure information. However, in many practical situations recommender systems need to be centered on implicit feedback. This may reflect reluctance of users to rate products, or limitations of the system that is unable to collect explicit feedback. In an implicit model, once the user gives approval to collect usage data, no additional explicit feedback (e.g. ratings) is required on the user’s part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import random\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_csv('events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1407580"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_customers = events_df.visitorid.unique()\n",
    "len(all_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a total of 1407580 unique visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11719"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_purchased = events_df[events_df.transactionid.notnull()].visitorid.unique()\n",
    "len(customer_purchased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 1407580 visitors, only 11719 ended up purchasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1395861"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_browsed = [x for x in all_customers if x not in customer_purchased]\n",
    "len(customer_browsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest 1395861 left without buying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have higly imbalanced and a large dataset here, processing them would have taken a LONG time.So, we are going to limit our dataset to a 50:50 mixture of data from both \"purchasing\" as well as \"just browsing\" visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_browsed = random.sample(customer_browsed,len(customer_purchased))\n",
    "sample = list(customer_purchased) + list(random_browsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter our datset to contain only those data instances that are in our calculated sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df[events_df['visitorid'].isin(sample)]\n",
    "events_df = events_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now map <i>visitorid</i> and <i>itemid</i> to values(categorical?) starting from zero, so they can be easily indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df['visitor_id'] = events_df['visitorid'].astype('category').cat.codes\n",
    "visitor_lookup = events_df[['visitor_id','visitorid']].drop_duplicates()\n",
    "events_df['item_id'] = events_df['itemid'].astype('category').cat.codes\n",
    "item_lookup = events_df[['item_id','itemid']].drop_duplicates()\n",
    "\n",
    "events_df.drop(['itemid','visitorid'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a sparse matrix that will store for a user it's browsing history, that is how many times the user has interacted with an item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors = list(np.sort(events_df.visitor_id.unique()))\n",
    "items = list(np.sort(events_df.item_id.unique()))\n",
    "\n",
    "mat = np.zeros((len(visitors),len(items)))\n",
    "\n",
    "events_df.head()\n",
    "\n",
    "for row in events_df.itertuples():\n",
    "    itmid = row.item_id\n",
    "    visid = row.visitor_id\n",
    "    mat[visid][itmid] += 1\n",
    "\n",
    "data_sparse = sparse.csr_matrix(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS\n",
    "\n",
    "Alternating Least Squares (ALS) is a the model we’ll use to fit our data and find similarities.\n",
    "\n",
    "#### Matrix factorization Implicit data\n",
    "The idea is basically to take a large (or potentially huge) matrix and factor it into some smaller representation of the original matrix. Doing this reduction and working with fewer dimensions makes it both much more computationally efficient and but also gives us better results since we can reason about items.\n",
    "\n",
    "There are different ways to factor a matrix, like Singular Value Decomposition (SVD) or Probabilistic Latent Semantic Analysis (PLSA) if we’re dealing with explicit data. With implicit data the difference lies in how we deal with all the missing data in our very sparse matrix. For explicit data we treat them as just unknown fields that we should assign some predicted rating to. But for implicit we can’t just assume the same since there is information in these unknown values as well. As stated before we don’t know if a missing value means the user disliked something, or if it means they love it but just don’t know about it. Basically we need some way to learn from the missing data.\n",
    "\n",
    "#### Back to ALS\n",
    "ALS is an iterative optimization process where we for every iteration try to arrive closer and closer to a factorized representation of our original data. We have our original matrix R of size (u x i) with our users, items and some type of feedback data. We then want to find a way to turn that into one matrix with users and hidden features of size (u x f) and one with items and hidden features of size (f x i). In U and V we have weights for how each user/item relates to each feature. What we do is we calculate U and V so that their product approximates R as closely as possible: R ≈ U x V.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](factor.png \"Title\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By randomly assigning the values in U and V and using least squares iteratively we can arrive at what weights yield the best approximation of R.\n",
    "With the alternating least squares approach we use the same idea but iteratively alternate between optimizing U and fixing V and vice versa. We do this for each iteration to arrive closer to R = U x V.\n",
    "\n",
    "The approach we’re going to use with our implicit dataset is the one outlined in [Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf) by Hu, Korenand and Volinsky (and used by Facebook and Spotify).\n",
    "\n",
    "Their solution is to merge the preference (p) for an item with the confidence (c) we have for that preference. We start out with missing values as a negative preference with a low confidence value and existing values a positive preference but with a high confidence value.\n",
    "\n",
    "* Preference\n",
    "\n",
    "![alt text](preference.png \"Title\")\n",
    "Basically our preference is a binary representation of our feedback data r. If the feedback is greater than zero we set it to 1.\n",
    "\n",
    "* Confidence\n",
    "\n",
    "![alt text](confidence.png \"Title\")\n",
    "Confidence is calculated using the magnitude of <i>r</i> (the feedback data) giving us a larger confidence the more times a user has played, viewed or clicked an item. The rate of which our confidence increases is set through a linear scaling factor <i>α</i>. We also add 1 so we have a minimal confidence even if α x r equals zero.\n",
    "\n",
    "The goal now is to find the vector for each user (xu) and item (yi) in feature dimensions which means we want to minimize the following loss function:\n",
    "![alt text](loss_function.png \"Title\")\n",
    "\n",
    "As the paper notes, if we fix the user factors or item factors we can calculate a global minimum. The derivative of the above equation gets us the following equation for minimizing the loss of our users:\n",
    "![alt text](user.png \"Title\")\n",
    "And the this for minimizing it for our items\n",
    "![alt text](item.png \"Title\")\n",
    "\n",
    "One more step is that by realizing that the product of Y-transpose, Cu and Y can be broken out as shown below:\n",
    "![alt text](broken_down.png \"Title\")\n",
    "Now we have Y-transpose-Y and X-transpose-X independent of u and i which means we can precompute it and make the calculation much less intensive. So with that in mind our final user and item equations are:\n",
    "![alt text](final_equations.png \"Title\")\n",
    "\n",
    "By iterating between computing the two equations above we arrive at one matrix with user vectors and one with item vectors that we can then use to produce recommendations or find similarities.\n",
    "\n",
    "#### Similar Items\n",
    "To calculate the similarity between items we compute the dot-product between our item vectors and it’s transpose. So if we want artists similar to say \"A\" we take the dot product between all item vectors and the transpose of the A's item vector. This will give us the similarity score:\n",
    "![alt text](similar_items.png \"Title\")\n",
    "\n",
    "#### Making Recommendations\n",
    "To make recommendations for a given user we take a similar approach. Here we calculate the dot product between our user vector and the transpose of our item vectors. This gives us a recommendation score for our user and each item:\n",
    "![alt text](recommendation.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brute implementation of the ALS, especially on a large datset as of ours, it will take a VERY long time.\n",
    "\n",
    "Following the implementation and code by [Ben Frederickson ](http://www.benfrederickson.com/fast-implicit-matrix-factorization/) we can replace our implicit_als function with the below code and speed things up quite a bit. Here we’re using the approach outlined in [this paper ](https://www.semanticscholar.org/paper/Applications-of-the-conjugate-gradient-method-for-Tak%C3%A1cs-Pil%C3%A1szy/46e905e9134e97c625ea6c8f6fa961b0b4c80fcf) using the Conjugate Gradient (CG) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzeros(m, row):\n",
    "    for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_als_cg(Cui, features=20, iterations=20, lambda_val=0.1):\n",
    "    user_size, item_size = Cui.shape\n",
    "\n",
    "    X = np.random.rand(user_size, features) * 0.01\n",
    "    Y = np.random.rand(item_size, features) * 0.01\n",
    "\n",
    "    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "    \n",
    "    def least_squares_cg(Cui, X, Y, lambda_val, cg_steps=3):\n",
    "        users, features = X.shape\n",
    "    \n",
    "        YtY = Y.T.dot(Y) + lambda_val * np.eye(features)\n",
    "\n",
    "        for u in range(users):\n",
    "\n",
    "            x = X[u]\n",
    "            r = -YtY.dot(x)\n",
    "\n",
    "            for i, confidence in nonzeros(Cui, u):\n",
    "                r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
    "\n",
    "            p = r.copy()\n",
    "            rsold = r.dot(r)\n",
    "\n",
    "            for it in range(cg_steps):\n",
    "                Ap = YtY.dot(p)\n",
    "                for i, confidence in nonzeros(Cui, u):\n",
    "                    Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
    "    \n",
    "                alpha = rsold / p.dot(Ap)\n",
    "                x += alpha * p\n",
    "                r -= alpha * Ap\n",
    "                rsnew = r.dot(r)\n",
    "                p = r + (rsnew / rsold) * p\n",
    "                rsold = rsnew\n",
    "                \n",
    "            X[u] = x\n",
    "            \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print ('iteration %d of %d' % (iteration+1, iterations))\n",
    "        least_squares_cg(Cui, X, Y, lambda_val)\n",
    "        least_squares_cg(Ciu, Y, X, lambda_val)\n",
    "    \n",
    "    return sparse.csr_matrix(X), sparse.csr_matrix(Y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 20\n",
      "iteration 2 of 20\n",
      "iteration 3 of 20\n",
      "iteration 4 of 20\n",
      "iteration 5 of 20\n",
      "iteration 6 of 20\n",
      "iteration 7 of 20\n",
      "iteration 8 of 20\n",
      "iteration 9 of 20\n",
      "iteration 10 of 20\n",
      "iteration 11 of 20\n",
      "iteration 12 of 20\n",
      "iteration 13 of 20\n",
      "iteration 14 of 20\n",
      "iteration 15 of 20\n",
      "iteration 16 of 20\n",
      "iteration 17 of 20\n",
      "iteration 18 of 20\n",
      "iteration 19 of 20\n",
      "iteration 20 of 20\n"
     ]
    }
   ],
   "source": [
    "alpha_val = 15\n",
    "conf_data = (data_sparse * alpha_val).astype('double')\n",
    "user_vecs, item_vecs = implicit_als_cg(conf_data, iterations=20, features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        item_id  itemid\n",
      "0         42601  455170\n",
      "56853     28614  305940\n",
      "71208     15572  167319\n",
      "199157    41429  442415\n",
      "242495     1899   20884\n"
     ]
    }
   ],
   "source": [
    "#Let's say we want recommendation for visitor with visitorid '1397760'\n",
    "vid = 1397760\n",
    "\n",
    "# Lt's get its visitor_id in the dataframe by consulting visitor_lookup\n",
    "v_id = visitor_lookup.loc[visitor_lookup.visitorid == vid]['visitor_id'].iloc[0]\n",
    "\n",
    "#Let's get items consunmed by user\n",
    "consumed_idx = data_sparse[v_id,:].nonzero()[1].astype(str)\n",
    "consumed_items = item_lookup.loc[item_lookup.item_id.isin(consumed_idx)]\n",
    "print(consumed_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now create user recommendation\n",
    "def recommend(vid, data_sparse, user_vecs, item_vecs, item_lookup, num_items=10):\n",
    "    \n",
    "    user_id = visitor_lookup.loc[visitor_lookup.visitorid == vid]['visitor_id'].iloc[0]\n",
    "    # Get all interactions by the user\n",
    "    user_interactions = data_sparse[user_id,:].toarray()\n",
    "\n",
    "    # We don't want to recommend items the user has consumed. So let's\n",
    "    # set them all to 0 and the unknowns to 1.\n",
    "    user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "    # This is where we calculate the recommendation by taking the \n",
    "    # dot-product of the user vectors with the item vectors.\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
    "\n",
    "    # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    recommend_vector = user_interactions*rec_vector_scaled\n",
    "   \n",
    "    # Get all the item indices in order of recommendations (descending) and\n",
    "    # select only the top \"num_items\" items. \n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "\n",
    "    items = []\n",
    "    scores = []\n",
    "\n",
    "    # Loop through our recommended items indicies and look up the actial artist name\n",
    "    for idx in item_idx:\n",
    "        items.append(item_lookup.loc[item_lookup.item_id == idx]['itemid'].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    # Create a new dataframe with recommended artist names and scores\n",
    "    recommendations = pd.DataFrame({'items': items, 'score': scores})\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend(1397760,data_sparse, user_vecs, item_vecs, item_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    items     score\n",
      "0  461686  1.000000\n",
      "1  218794  0.788879\n",
      "2  171878  0.724170\n",
      "3   10572  0.711860\n",
      "4   32581  0.616565\n",
      "5  450082  0.579289\n",
      "6  268883  0.566760\n",
      "7  420960  0.566450\n",
      "8  336842  0.527456\n",
      "9  285154  0.515774\n"
     ]
    }
   ],
   "source": [
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
