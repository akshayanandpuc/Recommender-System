{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking\n",
    "[BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper puts forth something they call “BRP-Opt”, a generic optimization criterion for optimal personalized ranking. Basically what this means is an approach that can be applied to different types of recommendation models like Matrix Factorization or k-Nearest-Neighbour (that's the “generic” part) and that solves a ranking for a set of items for a given user.\n",
    "\n",
    "Whereas most other optimizers just look at if a user interacted with an item, BPR looks at the user, one item the user interacted with and one item the user did not (the unknown item). This gives us a triplet **(u, i, j)** of a user **(u)**, one known item **(i)** and one unknown item **(j)**.\n",
    "\n",
    "We can express the relationship between known and unknown items like this:\n",
    "$$\\hat{x}_ui - \\hat{x}_uj > 0 $$\n",
    "\n",
    "Here $\\hat{x}$<sub>ui</sub> denotes the score for user 'u' and a known item 'i' and $\\hat{x}$<sub>uj</sub> the score for user 'u' and unknown item 'j'. The above condition is satisfied if the score for the known item is larger than that of the unknown one.\n",
    "\n",
    "### Bayesian Formulation\n",
    "\n",
    "Posterior probability is proportional to the likelihood multiplied by the prior probability:\n",
    "<i>Posterior probability ∝ Likelihood x Prior probability</i>\n",
    "\n",
    "So what we want to do is to use a formulation to update the probability of our hypothesis as more events/information becomes available, i.e we want to maximize the probability of it being true.\n",
    "\n",
    "### A Closer Look At The Math\n",
    "\n",
    "Using the formula <i>Posterior probability ∝ Likelihood x Prior probability</i> the posterior probability we want to maximize in this case is:\n",
    "\n",
    "$$p(\\theta | >_u) ∝ p(>_u | \\theta) x p(\\theta)$$\n",
    "Here $>_u$ is a latent preference structure for user **u** and $\\theta$ represents the parameters of some kind of model, like matrix factorization or kNN.\n",
    "\n",
    "So basically we want to maximize the probability of parameters $\\theta$ given a latent preference structure for user $>_u$. The paper shows us that the product of the likelihood $p(>_u | \\theta)$ is equal to the product of $p(i <_u j | \\theta)$:\n",
    "<img src=\"form1.png\"  style=\"width: 400px\">\n",
    "Next, we define the likelihood that a given user actually prefers the known item i over unknown item j as the following:\n",
    "<img src=\"form2.png\" style=\"width: 300px\">\n",
    "Here $\\sigma$ is the sigmoid function and $\\hat{x}_{uij}(\\theta)$ is some kind of function that models the relationship between a user, a known item and an unknown item given a set of parameters. BPR does not dictate what function this is and can, therefore, be used with a number of different model classes. In our case this function is the difference between the score of **u and j** subtracted from the score of **u and i**:\n",
    "<img src=\"form3.png\" style=\"width: 200px\">\n",
    "We can then rewrite it to use the sigmoid function for the likelihood. The paper also suggests using ln sigmoid (natural log) based on their MLE (Maximum Likelihood Evaluation):\n",
    "<img src=\"form4.png\" style=\"width: 300px\">\n",
    "Remembering the logarithm product rule ( ln(a * b) = ln(a) + ln(b) ) we can now change the whole equation to:\n",
    "<img src=\"form5.png\" style=\"width: 350px\">\n",
    "Based on this we then arrive at the final optimization criterion for our model:\n",
    "<img src=\"form6.png\" style=\"width: 300px\">\n",
    "* $\\theta$ : Our model parameter vector.\n",
    "* $\\hat{x}_{uij}$ : Relationship between (u, i, j). Here the score of (u, j) subtracted from the score of (u, j).\n",
    "* **ln** : The natural logarithm.\n",
    "* $\\sigma$ : The logistic sigmoid.\n",
    "* $\\lambda$ : LAmbda, the regularization hyperparameter for our model.\n",
    "* $||\\theta||$ : The L2 norm of our model parameters.\n",
    "\n",
    "### The Tensorflow Model\n",
    "\n",
    "Follolwing are some of the nodes we will be using when building our graph:\n",
    "* **Variables**: Nodes that maintain their state in the graph across multiple executions. These are the things in our graph we want to train in order to minimize the loss function.\n",
    "* **Embedding lookup**: Lets us retrieve a row from a tensor given its index. Similar to indexing into a numpy array.\n",
    "* **Optimizer**: There are different types of optimizers but what they do is to calculate the loss function and then update the values in the graph for the next run in order to minimize that loss.\n",
    "\n",
    "### The Model\n",
    "\n",
    "We will implement BPR using matrix factorization which means taking the interaction matrix <b>R</b> of size (all users x all items) and factoring it into matrix <b>U</b> of size (all users x latent features) and <b>V</b> of size (latent features x all items). So when we’re done want <b>R ≈ U x V</b>.\n",
    "<img src=\"matrix_fact.png\" style=\"width: 800px\">\n",
    "We will use Stochastic Gradient Descent (SGD) to arrive at this approximation. It works by initializing <b>U</b> and <b>V</b> with uniformly random values and then, using the optimization criterion from before, continuously updating them with new values to maximize the posterior probability i.e. getting a higher and higher probability for <b>R = U x V</b>.\n",
    "\n",
    "Let’s have a look at a simplified illustration of the graph we will be implementing and how data will flow through it.\n",
    "<img src=\"model.png\" style=\"width: 800px\">\n",
    "So we start at the top with our inputs (placeholders) which will be a user id <b>u</b>, a known item id <b>i</b> and unknown item id <b>j</b>. We then also randomly initialize our matrixes <b>U</b> and <b>V</b> (variables).\n",
    "\n",
    "From here we create three embeddings, one for the user factors, known item factors and unknown item factors. We take these embeddings and multiply them along with a bias to get $\\hat{x}_{ui}$ and $\\hat{x}_{uj}$ respectively.\n",
    "\n",
    "Next, we take $\\hat{x}_{ui}$ and $\\hat{x}_{uj}$, compute $\\hat{x}_{uij}$ and feed it into the negative natural log — of the sigmoid — of $\\hat{x}_{uij}$. So we get $-ln \\sigma(\\hat{x}_{uij})$  \n",
    "\n",
    "Finally, we add the <b>L2</b> norm to $-ln \\sigma(\\hat{x}_{uij})) and arrive at the final loss function:\n",
    "<img src=\"form7.png\" style=\"width: 200px\">\n",
    "\n",
    "Let’s expand this out to see it in the form it will actually be implemented in code:\n",
    "<img src=\"form8.png\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "As before we’ll be using the lastfm dataset containing the listening behavior of 360,000 users. It contains the user id, an artist id, the name of the artists and the number of times a user played any given artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('usersha1-artmbid-artname-plays.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['user','artistid','artist','plays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>plays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>die Ärzte</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>melissa etheridge</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>elvenking</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>juliette &amp; the licks</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>red hot chili peppers</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user                 artist  plays\n",
       "0  00000c289a1829a808ac09c00daf10bc3c4e223b              die Ärzte   1099\n",
       "1  00000c289a1829a808ac09c00daf10bc3c4e223b      melissa etheridge    897\n",
       "2  00000c289a1829a808ac09c00daf10bc3c4e223b              elvenking    717\n",
       "3  00000c289a1829a808ac09c00daf10bc3c4e223b   juliette & the licks    706\n",
       "4  00000c289a1829a808ac09c00daf10bc3c4e223b  red hot chili peppers    691"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['artistid'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Convert artists names into numerical IDs\n",
    "df['user_id'] = df['user'].astype(\"category\").cat.codes\n",
    "df['artist_id'] = df['artist'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_lookup = df[['artist_id', 'artist']].drop_duplicates()\n",
    "item_lookup['artist_id'] = item_lookup.artist_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['user', 'artist'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with 0 plays\n",
    "df = df.loc[df.plays != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of all users, artists and plays\n",
    "users = list(np.sort(df.user_id.unique()))\n",
    "artists = list(np.sort(df.artist_id.unique()))\n",
    "plays = list(df.plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows and columns for our new matrix\n",
    "rows = df.user_id.astype(float)\n",
    "cols = df.artist_id.astype(float)\n",
    "\n",
    "# Contruct a sparse matrix for our users and items containing number of plays\n",
    "data_sparse = sp.csr_matrix((plays, (rows, cols)), shape=(len(users), len(artists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids, iids = data_sparse.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batches = 30\n",
    "num_factors = 64 # Number of latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent lambda regularization values \n",
    "# for user, items and bias.\n",
    "lambda_user = 0.0000001\n",
    "lambda_item = 0.0000001\n",
    "lambda_bias = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our learning rate \n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many (u,i,j) triplets we sample for each batch\n",
    "samples = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create one function that just initializes a Tensorflow variable with random values, one that uses that function to create a variable and then embeds it using an embedding lookup and one for getting the value of a variable back given its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variable(size, dim, name=None):\n",
    "    # Helper function to initialize a new variable with uniform random values.\n",
    "    std = np.sqrt(2 / dim)\n",
    "    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(inputs, size, dim, name=None):\n",
    "    # Helper function to get a Tensorflow variable and create an embedding lookup to map our user and itemindices to vectors.\n",
    "    emb = init_variable(size, dim, name)\n",
    "    return tf.nn.embedding_lookup(emb, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(graph, session, name):\n",
    "    # Helper function to get the value of a Tensorflow variable by name.\n",
    "    v = graph.get_operation_by_name(name)\n",
    "    v = v.values()[0]\n",
    "    v = v.eval(session=session)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create placeholders for u, i and j that we can later feed data into. We initialize our *U* and *V* matrixes and create embeddings for *u_factors, i_factors*, and *j_factors*. We also create embeddings for our biases *i_bias* and *j_bias*.\n",
    "\n",
    "Next, we calculate the score for our user and item *i* as well as for item *j* to get x̂ui and x̂uj and then we compute *x̂uij*.\n",
    "\n",
    "Here we also the AUC score and add it to our Tensorflow summary so we can monitor it during training.\n",
    "\n",
    "For the L2 norm, all we need to do is to square each parameter multiplied with the regularization value and then add them all together. Taking the negative natural log of the sigmoid of *x̂uij*, plus the L2 norm gives us our final loss function. Lastly, we use the built-in Adam optimizer to minimize our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-f2220bbc0801>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-f2220bbc0801>\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    tf.summary.scalar(u_auc)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    #Loss function: \n",
    "    #-SUM ln σ(xui - xuj) + λ(w1)**2 + λ(w2)**2 + λ(w3)**2 ...\n",
    "    #ln = the natural log\n",
    "    #σ(xuij) = the sigmoid function of xuij.\n",
    "    #λ = lambda regularization value.\n",
    "    #||W||**2 = the squared L2 norm of our model parameters.\n",
    "    \n",
    "    # Input into our model, in this case our user (u),\n",
    "    # known item (i) an unknown item (i) triplets.\n",
    "    u = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    i = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    j = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    \n",
    "    # User feature embedding\n",
    "    u_factors = embed(u, len(users), num_factors, 'user_factors') # U matrix\n",
    "\n",
    "    # Known and unknown item embeddings\n",
    "    item_factors = init_variable(len(artists), num_factors, \"item_factors\") # V matrix\n",
    "    i_factors = tf.nn.embedding_lookup(item_factors, i)\n",
    "    j_factors = tf.nn.embedding_lookup(item_factors, j)\n",
    "\n",
    "    # i and j bias embeddings.\n",
    "    item_bias = init_variable(len(artists), 1, \"item_bias\")\n",
    "    i_bias = tf.nn.embedding_lookup(item_bias, i)\n",
    "    i_bias = tf.reshape(i_bias, [-1, 1])\n",
    "    j_bias = tf.nn.embedding_lookup(item_bias, j)\n",
    "    j_bias = tf.reshape(j_bias, [-1, 1])\n",
    "\n",
    "    # Calculate the dot product + bias for known and unknown\n",
    "    # item to get xui and xuj.\n",
    "    xui = i_bias + tf.reduce_sum(u_factors * i_factors, axis=2)\n",
    "    xuj = j_bias + tf.reduce_sum(u_factors * j_factors, axis=2)\n",
    "    \n",
    "    # We calculate xuij.\n",
    "    xuij = xui - xuj\n",
    "\n",
    "    # Calculate the mean AUC (area under curve).\n",
    "    # if xuij is greater than 0, that means that \n",
    "    # xui is greater than xuj (and thats what we want).\n",
    "    #u_auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "    if xuij>0:\n",
    "        u_auc = tf.reduce_mean(tf.cast(xuij,tf.float32)\n",
    "    \n",
    "    # Output the AUC value to tensorboard for monitoring.\n",
    "    tf.summary.scalar(u_auc)\n",
    "\n",
    "    # Calculate the squared L2 norm ||W||**2 multiplied by λ.\n",
    "    l2_norm = tf.add_n([\n",
    "        lambda_user * tf.reduce_sum(tf.multiply(u_factors, u_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(i_factors, i_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(j_factors, j_factors)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(j_bias, j_bias))\n",
    "        ])\n",
    "    # Calculate the loss as ||W||**2 - ln σ(Xuij)\n",
    "    #loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    loss = -tf.reduce_mean(tf.log(tf.sigmoid(xuij))) + l2_norm\n",
    "    \n",
    "    # Train using the Adam optimizer to minimize \n",
    "    # our loss function.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss)\n",
    "\n",
    "    # Initialize all tensorflow variables.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch and batch we defined in our hyperparameters we sample a number of random indices from our dataset and then get user ids (u) and known item ids (i) matching those indices. We then sample 15 000 random unknown items (j).\n",
    "\n",
    "These items go into our feed_dict which is a dictionary we will feed into our graph. We then run the graph with those values, update the progress bar and display the current loss and AUC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Unsupported feed type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Unsupported feed type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-65a74f6b41fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# We run the session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_auc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Unsupported feed type"
     ]
    }
   ],
   "source": [
    "# Run the session. \n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "# This has noting to do with tensorflow but gives\n",
    "# us a nice progress bar for the training.\n",
    "progress = tqdm(total=batches*epochs)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for _ in range(batches):\n",
    "\n",
    "        # We want to sample one known and one unknown \n",
    "        # item for each user. \n",
    "\n",
    "        # First we sample 15000 uniform indices.\n",
    "        idx = np.random.randint(low=0, high=len(uids), size=samples)\n",
    "\n",
    "        # We then grab the users matching those indices.\n",
    "        batch_u = uids[idx].reshape(-1, 1)\n",
    "\n",
    "        # Then the known items for those users.\n",
    "        batch_i = iids[idx].reshape(-1, 1)\n",
    "        \n",
    "        # Lastly we randomly sample one unknown item for each user.\n",
    "        batch_j = np.random.randint(low=0, high=len(artists), size=(samples, 1), dtype='int32')\n",
    "\n",
    "        # Feed our users, known and unknown items to\n",
    "        # our tensorflow graph. \n",
    "        feed_dict = { u: batch_u, i: batch_i, j: batch_j }\n",
    "\n",
    "        # We run the session.\n",
    "        var, l, auc = session.run([step, loss, u_auc], feed_dict)\n",
    "    \n",
    "    progress.update(batches)\n",
    "    progress.set_description('Loss: %.3f | AUC: %.3f' % (l, auc))\n",
    "\n",
    "progress.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
